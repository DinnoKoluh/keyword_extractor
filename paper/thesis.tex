\documentclass[12pt]{article}
\usepackage[utf8]{inputenc} %mogu sa ovim pisati ščđžć
\usepackage{extsizes} %mogu koristi visinu fonta koju želim 	%

%\usepackage{hyperref} 					
\usepackage{hhline}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{listings}
%\usepackage{xcolor}
\lstset { %
	language=C++,breaklines=true,showstringspaces=false,
	backgroundcolor=\color{black!5}, % set backgroundcolor
	keywordstyle=\color{red},
	commentstyle=\color{blue},
	basicstyle=\sffamily,% basic font setting
	numbers=left,%
	numberstyle={\tiny \color{black}},% size of the numbers
	numbersep=9pt, % this defines how far the numbers are from the text
}

\usepackage{times}

\usepackage{geometry} %štimanje margina
\geometry{textwidth=18cm}
\geometry{textheight=22cm}

\usepackage[english]{babel}

\usepackage{array} %za tabele
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


\usepackage{enumitem}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{multicol} % multikolone
\usepackage{graphicx}
\usepackage{setspace}

\usepackage[obeyspaces]{url} % for adding paths to a folder
\graphicspath{{./Figures/}}

%\path{{./Data/}} % adddig a path to a folder

\usepackage{wrapfig} %wraps figure
\usepackage{lipsum}  %fills text around figure

\usepackage{bm}  %mogu pisati boldirane formule sa simbolima

\usepackage{amssymb} % za pisannje skupova

\usepackage[usestackEOL]{stackengine} %za piasanje front page-a
\numberwithin{equation}{section}

%\makeatletter
%\renewcommand{\@seccntformat}[1]{}
%\makeatother  % removes numbers from sections but keeps the references
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\scriptsize \hspace*{0cm}  Thesis topic: Graph-Based Keyword Extraction from
 \\ Scientific Paper Abstracts using Word Embeddings}
\lhead{\scriptsize Universit\`a  di Bologna \\ Department of Computer Science and Engineering}
\fancyheadoffset{1cm} % to offset the header by some distance 


\usepackage{epsfig}
\usepackage{multirow}
\usepackage{pdfpages}
\setcounter{MaxMatrixCols}{20} % maksimalan broj kolona na 20
\usepackage{float} % da slike nisu poremećene
\usepackage[figurename=Figure]{caption} % mijenja ime sa figure u slika
\usepackage[tablename=Table]{caption} % mijenja ime sa figure u slika

\usepackage{chngcntr}
%\counterwithout{figure}{chapter} % stavlja bojeve unutar captiona slike

\usepackage{colortbl} % color in tables

\addto\captionsenglish{% Replace "english" with the language you use
	\renewcommand{\contentsname}% Promjena naziva content
	{Content}%
}

\usepackage{etoolbox}
%\patchcmd{\thebibliography}{\section*{\refname}}{}{}{} % smakinje ime references

%\usepackage[colorlinks=true, linkcolor = cyan]{hyperref}
\usepackage{hyperref}

%rotating picture
\usepackage{lscape}
\usepackage{rotating}

% tilda
\usepackage{undertilde}

%%coloring links
%\usepackage{hyperref}
%\hypersetup{
	%	colorlinks = true,
	%	linkbordercolor = [white],
	%	linkcolor = [magenta]
	%}

%\usepackage[colorlinks]{hyperref}
%\hypersetup{colorlinks=true, linkcolor=cyan,linkbordercolor=red}

%matlab
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


\lstset{
	language=Matlab,
	basicstyle=\footnotesize\ttfamily,
	breaklines=true,%
	morekeywords={matlab2tikz},
	keywordstyle=\color{blue},%
	morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
	identifierstyle=\color{black},%
	stringstyle=\color{mylilas},
	commentstyle=\color{mygreen},%
	showstringspaces=false,%without this there will be a symbol in the places where there is a space
	numbers=left,%
	numberstyle={\tiny \color{black}},% size of the numbers
	numbersep=9pt, % this defines how far the numbers are from the text
	emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
	emph=[2]{word1,word2}, emphstyle=[2]{style},   
}

\addto\captionsenglish{% Replace "english" with the language you use
	\renewcommand{\contentsname}% Promjena naziva content
	{Contents}%
}

% stil numerisanja stranica
\usepackage{lastpage}
\cfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}

\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{} % smakne ime references

% za pisanje pseudokoda
\usepackage[plain]{algorithm2e}
%\usepackage{algorithmic}
%\usepackage{algpseudocode}
%\usepackage{algorithmicx}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\algnewcommand\algorithmicinput{\textbf{Function:}}
%\algnewcommand\Function{\item[\algorithmicinput]}
%\usepackage{capt-of}
\SetAlgorithmName{Pseudocode}

%\renewcommand{\@algocf@capt@plain}{top}% formerly {bottom}

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
		\begin{center}
			\refstepcounter{algorithm}% New algorithm
			\hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
			\renewcommand{\caption}[2][\relax]{% Make a new \caption
				{\raggedright\textbf{\fname@algorithm~\thealgorithm} ##2\par}%
				\ifx\relax##1\relax % #1 is \relax
				\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
				\else % #1 is not \relax
				\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
				\fi
				\kern2pt\hrule\kern2pt
			}
		}{% \end{breakablealgorithm}
		\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
	\end{center}
}
\makeatother

\makeatletter
\newenvironment{breakalgo}[2][alg:\thealgorithm]{%
	\def\@fs@cfont{\bfseries}%
	\let\@fs@capt\relax%
	\par\noindent%
	\medskip%
	\rule{\linewidth}{.8pt}%
	\vspace{-3pt}%
	\captionof{algorithm}{#2}\label{#1}%
	\vspace{-1.7\baselineskip}%
	\noindent\rule{\linewidth}{.4pt}%
	\vspace{-1.3\baselineskip}%
}{%
	\vspace{-.75\baselineskip}%
	\rule{\linewidth}{.4pt}%
	\medskip%
}
\makeatother


\captionsetup{font=small}
\newcommand*{\matlab}{\textsc{Matlab}}
\newcommand*{\altmatlab}{{\mdseries\matlab}} 

\usepackage{xcolor}

\newsavebox{\bmatrixbox}
\newenvironment{colorbmatrix}
{\begin{lrbox}{\bmatrixbox}
		\mathsurround=0pt
		$\displaystyle
		\begin{bmatrix}}
		{\end{bmatrix}$%
	\end{lrbox}%
	\usebox{\bmatrixbox}%
	\kern-\wd\bmatrixbox
	\makebox[0pt][l]{$\left[\vphantom{\usebox{\bmatrixbox}}\right.$}%
	\kern\wd\bmatrixbox
}

\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}


\begin{document}
	
	\begin{titlepage}
		\begin{center}
			
			{\Large ALMA MATER STUDIORUM \\ UNIVERSIT\`A  DI BOLOGNA \\}
			\vspace{0.4cm} 
			\hrule
			\vspace{0.4cm} 
			{\Large DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING \\} 
			{\Large ARTIFICIAL INTELLIGENCE \\}
			\vspace{0.4cm} 
			\hrule
			\vspace{1cm}
			{\Large MASTER THESIS}\\[0.4cm]
			{\large in} \\[0.4cm]
			{\Large Natural Language Processing}\\[1cm]
			
			\hrule
			\vspace{0.4cm}
			
			{\Huge Graph-Based Keyword Extraction from}\\[0.3cm]
			{\Huge Scientific Paper Abstracts using Word Embeddings}
			\vspace{0.4cm}
			
			\hrule
			\vspace{2cm}
			
			{\large Author:}\\
			{\large Dinno Koluh\\}
			
			\vspace{2cm}
			
			{\large Supervisor:}\\[0.1cm]
			{\large Prof. Paolo Torroni}\\[0.4cm]
			
			{\large Co-Supervisor:}\\[0.1cm]
			{\large Dr. Federico Ruggeri}
			
			\vspace{2cm} 
			{\large Bologna,}\\[0.1cm] 
			{\large October 2023.}
			
		\end{center}
	\end{titlepage}
	\newpage
	\newgeometry{textwidth=16cm, textheight=22cm}
	\thispagestyle{empty}
	
	\begin{spacing}{1.5}
	\section*{Abstract}
	In the era of information overload it became essential to efficiently extract concise, precise and quality information from large texts. One aspect of information extraction is keyword extraction where large texts are represented as sets of tokens i.e. keywords. This prospect of keyword extraction is paramount to researchers as they deal with huge numbers of scientific papers, and having a good and concise representation of those papers is essential for them. This thesis paper addresses that problem in the realm of natural language processing (NLP). \\
	Using core concepts of NLP and modeling texts as graphs, we are going to build a model for the automatic extraction of keywords. This is done in an unsupervised manner as the importance of a word is calculated through the position and weights associated with respective words in the graph. The first metric used to calculate the graph weights are co-occurrence matrices and the other metric are word embeddings. Word embeddings became a crucial way of representing the semantic information of words as dense vectors. \\
	The results of this paper were compared with keywords that were provided by authors of scientific papers in the area of computer science which act as the ground truth, but crucially are not a component in the model construction, but just serve as a verifier of the model's accuracy.
	
	\textbf{Keywords}: NLP, keyword extraction, scientific papers, graphs, word-embeddings
	
	\newpage
	\pagestyle{empty}
	\tableofcontents
	\setcounter{page}{0}
	\newpage
	%\renewcommand{\listfigurename}{Figures}
	\listoffigures
	\setcounter{page}{0}
	\pagebreak
	
	%\renewcommand{\listtablename}{Popis tabela}
%	\listoftables
%	\thispagestyle{empty}
%	\setcounter{page}{0}
%	\pagebreak
	
	\newpage
	\pagestyle{fancy}
	%\thispagestyle{empty}
	\section{Introduction and Motivation}
	Natural Language Processing (NLP) is a subfield or Artificial Intelligence (AI) and linguistics that has a focus on the interaction between computers and human languages. This is mostly restricted to written language as other fields (like Speech Processing) deal with spoken language (using audio instead of textual features). In the past decade NLP has seen huge attention with the introduction of some key concepts like \textit{word-embeddings} \cite{word-embedding-survey} (which we are going to use) and \textit{transformers} \cite{transformers-survey}. At the moment of writing of this paper, NLP is the subfield of AI that has the most resources invested into it, mostly in research and development, with new large language models (LLMs) coming out on a daily basis. Our focus will be a bit shifted from Deep Neural Network (DNN) models and more to traditional Machine Learning (ML) models. \\
	NLP has many subfields and application areas such as:
	\begin{itemize}
		\item Text classification
		\item Information retrieval
		\item Automatic translation
		\item Speech analysis 
		\item Question answering
		\item Conversational agents
		\item Sentiment analysis
	\end{itemize} 
	The field we are going to be working on will be \textbf{information retrieval}, more precisely \textit{keyword extraction}. Keyword (keyphrase) extraction is the automatic selection of important and topical phrases from the body of a document \cite{keyword-extraction-0}. Scientific papers are usually the area where keywords are frequently used as researchers use them for a quick overview of papers and also sorting papers into different categories. This is the topic we are going to be working on as well. The keywords are going to be extracted from the abstracts of the scientific papers. One detail we should address that will be important later is the distinction between \textit{keywords} and \textit{keyphrases}. Keywords would be single words or at most MWEs (Multi-Word expressions, i.e. "deep learning") while keyphrases are more complicated entities comprised of several words and they act as a single unit (e.g. the phrase "scientific paper" would be a keyphrase). When doing keyword extraction we might have as an output a combination of both, keywords and keyphrases. Usually keyphrases carry more information than keywords but a combination of both as an output is most representative. From now on, when referring to \textit{keywords} it will also include \textit{keyphrases}, if not explicitly stated otherwise. \\
	The model to be used for keyword extraction is \textbf{graph-based}. The motivation behind using graphs is that graphs are a well-studied and understood concept and many practical problems can be represented with graphs. There are also a plethora of algorithms which are going to be useful for the case of instance ranking. \\
	The idea is to model words in a paper abstract as nodes of a graph. Not all the words in the abstract should be included, as keywords are usually composed of a combination of nouns and adjectives (e.g. scientific[ADJ] paper[N]). This means that some preprocessing of the raw text will be required, especially when dealing with MWEs. We will speak about this in the next chapter. \\
	As stated, the nodes of the graph will be modeled as words, but the question comes on how to model the edges of the graph? \\
	It starts from the assumption that words that occur in the same context tend to carry a similar meaning. The idea is to use a \textbf{sliding window} of some predefined size and words that are in that window are connected with and edge. In that way the final graph carries semantic (the meaning of words) information of the input text. The number of occurrences when sliding the window over the entire text will give us the initial graph weights. To solidify this relationship another metric that we will use are word embeddings. Word embeddings are essentially a way of representing words as vectors of numbers. We will dive more deeply into how word embeddings are computed ans used but for now we can assume that word embeddings are vectors of numbers and that these vectors carry semantic information of the corresponding word. This means that words that have a similar meaning (whatever that might mean in some general picture) also have similar vector representations and that we can use the usual mathematical tools on these vectors like the dot product which means that we can actually measure the similarity between words. \\
	We now have the complete representation of the input text as a graph. Now we need to somehow rank the nodes according to the graph edges and weights. We can refer to the ranking procedure as to finding the importance of each node. There are several algorithms which can find the importance of nodes, and we will speak in more detail about them later, but for now can assume we have a black box which takes in the graph representation of the abstract as described before and gives us all the nodes (word) ranked by their importance. \\
	We now have a list of the most important words, but there is one more step that we can do to get a more robust output. The given output would be a list of words, but we might want to have phrases instead of keywords (e.g. the phrase "scientific paper" is more representative instead of just "paper"). We can use the words we got out from the graph and traverse the initial text for phrases in which they appear. Then based on those phrases in the initial text and the importance of the words which they are made of, we can get alongside the ranked words also the phrases in which they appear. In this way we can also generalize the problem by letting a phrase be only made up of one keyword. If it is made up of more than one keyword we can average it out, and in the end get the ranking of the specific phrases which appear in the input text. \\
	This was a rough explanation of the procedures which should give us an overview of the steps involved in developing a pipeline for the extraction of keywords from paper abstracts. In the next few chapters we are going to look in more detail in the inner workings of the steps involved. We will start with the NLP preprocessing pipeline.
	
	
	\newpage
	\section{NLP Pipeline}
	In this chapter we are going to explain the preprocessing steps needed to transform the raw input text into something that can be modeled with a graph. These preprocessing steps are going to be carried out with concepts from NLP like tokenization, sentence splitting, lemmatization, etc. All these NLP concepts used for preprocessing can be stacked into a pipeline hence the name of the chapter, \textbf{NLP pipeline}.\\
	We will explain these concepts giving examples and at the end show the whole pipeline as a schema. In the end we will have single instances of processed words which are going to be the graph nodes. Let us start!
	
	\subsection{Tokenization}
	Tokenization is the basic and most important NLP concept. Let us firstly define what is a token. A \textbf{token} is a sequence of characters grouped together as a semantic unit used for processing \cite{stanford_NLP}. Depending on the use case,  tokens can be equivalent to words, but they don't have to. An example of words and tokens not being equivalent is:
	\begin{center}
		I'd $\rightarrow$ [I, 'd]
	\end{center}
	In this case the word "I'd" was broken into two tokens. Depending on the specific tokenization method used, words can be broken into lower units (subwords):
	\begin{center}
		unlucky $\rightarrow$ [un, lucky]
	\end{center}
	Here, we have that the adjective "unlucky" was broken into its prefix "un" and the adjective "lucky". In this example tokenization provided a way to work with text on a granular level, breaking down words into units which might not even be real words, but in this way we are making text processing tasks more cohesive for machines. \\
	The process of tokenization would be the process of breaking down an input text into single tokens. As the input text inherently has whitespaces and newlines, those are usually excluded from the obtained tokens. We can look at an example sentence broken down into tokens: 
	\begin{center}
		She didn't have time. $\rightarrow$ [She, did, n't, have, time, .]
	\end{center}
	When dealing with whitespaces, tokenization is a trivial process, but when dealing with punctuations, tokenization becomes a hard problem. The reason is that sometimes punctuations should be kept, but sometimes they should be separated. In the example above, the token "time." was separated into "time" and ".", but let's take a look at this example:
	\begin{center}
		She didn't have time for Mr. Nobody. $\rightarrow$ [She, did, n't, have, time, for, Mr., Nobody, .]
	\end{center}
	In this example the period for the token "Mr." was not separated while for "Nobody." it was. This is the reason why tokenization can be ambiguous and hard to compute correctly. And there are tens of other example os ambiguities apart from punctuation marks. One solution to the problem we've seen is to keep a \textbf{lexicon} of possible ambiguous tokens so we know how to separate them.
	
	\subsection{Sentence splitting}
	Sentence splitting is the process of splitting up a text into sentences. This steps is going to be important when computing co-occurrence matrices with the sliding window. The reason is that when using the sliding window we don't want to capture the end of one sentence and beginning of the next one in the same context as they might not carry the same context as sentences are naturally boundaries that also we, humans, respect and take into account when reading and processing text. \\
	Sentence splitting has inherently similar problems as does tokenization as punctuation signs can be ambiguous depending on the position and the tokens surrounding the punctuation sign. 
	
	\subsection{MWE}
	MWE or Multi-Word Expressions are sequences of words that when grouped together carry a meaning, but when standing alone do not. An example of a MWE is "deep learning". We see that the word "deep" and "learning" on their own carry other meanings in contrast when putting the together into a single phrase. For the purposes of this paper we need to keep these expressions together and not split them. The reason is that we are extracting keywords and keyphrases, and the keyphrase to be extracted from a paper that is written on the topic of "deep learning" is "deep learning", not "deep" and "learning". \\
	We expect when dealing with scientific papers, that they are going to have many MWE and possibly new, non-existent MWE. This makes the tokenization problem ever more harder because these tokens should be kept together. One solution is to again keep a MWE lexicon. Another solution, which was found after an inspection of the dataset is that novel MWE usually are written with upper-case letters and they have an abbreviation after the MWE in parenthesis (e.g. "Introduction to Large Language Models (LLMs)") and this fact can be leveraged when dealing with MWE that are not present in the MWE lexicon. \\
	Let us see an example for MWE tokenization:
	\begin{center}
		We are discussing Machine Learning (ML) models in San Francisco. $\rightarrow$ \\
		
		[We, are, discussing, Machine Learning, (, ML, ), models, in, San Francisco, .]
	\end{center} 
	
	\subsection{Token removal based on PoS tags}
	This step is specific for our task, but PoS (Part of Speech) is a regular NLP concept. \textbf{Part of Speech} tagging is the process of giving a grammatical category tag to words based on their syntactic and semantic meaning. Basically this step assigns syntactic tags to words i.e. is a word a noun, verb, adjective, etc. The question is why do we need this step?\\
	The reason is the nature of the task we are doing. Namely, keywords are usually a phrase of an adjective and verb or at most a single noun. Taking this into consideration we can substantially reduce the graph size if we just keep nouns and adjective as the possible graph nodes. There are several ways on getting PoS tags of words, like rule-based methods, statistical methods, hybrid methods, deep learning methods, etc. 
	We are not going into the inner workings of these methods, but just use an existent model for getting PoS tags. One of the problems when computing PoS tags is they highly depend on the context in which they occur as we can have words which are the same but carry different PoS tags. For example the word "developed" can be a verb and an adjective at the same time as show below:
	\begin{center}
		Japan[Noun] developed[Verb] into[Prep.] a[Det.] developed[Adj.] nation[Noun].
	\end{center}
	This is the reason why context is very important when assigning PoS tags. Deep learning and statistical methods are usually good at these context specific examples but they require quite large training datasets. \\
	An example of this step can be seen below:
	\begin{center}
		Japan developed into a developed nation. $\rightarrow$ [Japan, developed, nation, .]
	\end{center}	  
	
	\subsection{Stopword removal}
	Stopwords are common words occurring in a language that are usually filtered out during NLP tasks as they do not carry much value for the specific task. For our purposes stopword removal is essential as we are going to have many of them, and in the final ranking they will not be present in the keyphrases. The usual method of stopword removal is to build a stopword lexicon and just remove those words from the input text. An example of stopwords are: "the", "and", "in", etc. Let us see an example for stopword removal on a sentence:
	\begin{center}
		The quick brown fox jumped over the lazy dog. $\rightarrow$ [quick, brown, fox, jumped, lazy, dog, .]
	\end{center} 
	This step will come after the token removal based on PoS tags, as stopwords could carry context specific information for PoS tagging. Even though PoS tagging should take care of stopwords, we will still include this step in case some stopword slip thorough the previous step, as stopword removal is a cheap operation.
	
	\subsection{Lemmatization}
	
	\subsection{Normalization}
	
	 
	 
	 
	 
	 
	 
	 
	\newpage 
	\section{Word Embeddings}
	
	\section{Graph Construction}
	
	\section{Implementation of Keyword Extraction}
	
	\section{Testing, Results and Discussion}
	
	\section{Conclusion}
	
	\newpage
	\section{Literature}
	
	\begin{thebibliography}{8}
		\bibitem{word-embedding-survey}
		Almeida, Felipe, and Geraldo Xexéo. "\textit{Word embeddings: A survey}.", arXiv preprint arXiv:1901.09069 (2019). \\
		\texttt{URL: }\url{https://arxiv.org/pdf/1901.09069.pdf}
		
		\bibitem{transformers-survey}
		Lin, Tianyang, et al. "\textit{A survey of transformers}.", AI Open (2022).\\
		\texttt{URL: }\url{https://www.sciencedirect.com/science/article/pii/S2666651022000146}
		
		\bibitem{keyword-extraction-0}
		Zu, Xian, Fei Xie, and Xiaojian Liu. "\textit{Graph-based keyphrase extraction using word and document embeddings}.", 2020 IEEE International Conference on Knowledge Graph (ICKG). IEEE, 2020. \\
		\texttt{URL: }\url{https://ieeexplore.ieee.org/abstract/document/9194571}
		
		\bibitem{stanford_NLP}
		The Stanford NLP Group\\
		\texttt{URL: }\url{https://nlp.stanford.edu/}
		
		\bibitem{juricGrafovi}
		Jurić Ž. "\textit{Diskretna matematika za studente tehničkih nauka}". ETF Sarajevo, UNSA, 2017. 
		
	\end{thebibliography}

	\end{spacing}
	
\end{document}