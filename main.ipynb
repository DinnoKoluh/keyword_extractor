{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nlp_utils import *\n",
    "from data_utils import *\n",
    "from KeywordExtractor import *\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "methods = {1: \"degree_centrality\", 2: \"closeness_centrality\", 3: \"betweenness_centrality\", 4:\"eigenvector_centrality\", 5:\"pagerank\"} #, \\\n",
    "           #6:\"katz_centrality\"} #, 7: \"hits\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD EMBEDDINGS\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "# Download a pre-trained Word2Vec model (you can choose other models as well)\n",
    "model_name = \"glove-wiki-gigaword-50\"\n",
    "\n",
    "# Download and load the model (this might take a while to download)\n",
    "# model = api.load(model_name)\n",
    "# model.save('data/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['world war 2', 'b', 'c', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "Precision: 0.3333 \n",
    "Recall: 0.4000 \n",
    "F-score: 0.3636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5000 \n",
      "Recall: 0.6000 \n",
      "F-score: 0.5455\n"
     ]
    }
   ],
   "source": [
    "true_labels = ['Worlds', 'b', 'c', 'd', 'e']\n",
    "predicted_labels = ['world_war_2', 'c', 'j', 'k', 'e', 'op']\n",
    "p, r, f = get_prf(true_labels, predicted_labels)\n",
    "print(f'Precision: {p:.4f} \\nRecall: {r:.4f} \\nF-score: {f:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1. abstract\n",
      "Finished 11. abstract\n",
      "Finished 21. abstract\n",
      "Finished 31. abstract\n",
      "Finished 41. abstract\n",
      "Finished 51. abstract\n",
      "Finished 61. abstract\n",
      "Finished 71. abstract\n",
      "Finished 81. abstract\n",
      "Failed on abstract: 84, \n",
      " error: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')\n",
      "Finished 91. abstract\n",
      "Finished 101. abstract\n"
     ]
    }
   ],
   "source": [
    "path_to_file = 'data/CS_subset_20.csv' # 'data/CS_papers.csv'\n",
    "path_to_file = 'data/CS_subset.csv'\n",
    "\n",
    "#joined, abstracts, titles, keywords = get_data(path_to_file, version=\"CS\")\n",
    "#print(len(joined))\n",
    "metrics_dict = make_keyword_metrics(methods, path_to_file, window_size=5)\n",
    "# print(joined[0])\n",
    "# print(abstracts[0])\n",
    "# print(titles[0])\n",
    "# print(keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For degree_centrality_p the mean is: 0.30757039989713253\n",
      "For degree_centrality_r the mean is: 0.34512022630834516\n",
      "For degree_centrality_f the mean is: 0.3252243327282409\n",
      "For closeness_centrality_p the mean is: 0.260997171145686\n",
      "For closeness_centrality_r the mean is: 0.2922049347791922\n",
      "For closeness_centrality_f the mean is: 0.27568540544048614\n",
      "For betweenness_centrality_p the mean is: 0.2977562041918478\n",
      "For betweenness_centrality_r the mean is: 0.33393053591073396\n",
      "For betweenness_centrality_f the mean is: 0.31476711580932526\n",
      "For eigenvector_centrality_p the mean is: 0.2685194805194805\n",
      "For eigenvector_centrality_r the mean is: 0.3007142857142857\n",
      "For eigenvector_centrality_f the mean is: 0.28367071524966264\n",
      "For pagerank_p the mean is: 0.3262794136556513\n",
      "For pagerank_r the mean is: 0.3660537482319662\n",
      "For pagerank_f the mean is: 0.3449827920906607\n"
     ]
    }
   ],
   "source": [
    "for key in metrics_dict.keys():\n",
    "    print(f\"For {key} the mean is: {np.mean(np.array(metrics_dict[key]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"The island country of Japan has developed into a great economy after World War 2. \\\n",
    "The Japan sea is a source of fish. Sushi is a famous fish and rice food.\"\n",
    "tokens, sentences = prune_text(abstract)\n",
    "co, index_dict = get_co(sentences, representation='matrix')\n",
    "print(index_dict)\n",
    "print(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island country of Japan has developed into a great economy after World War 2. The Japan sea is a source of fish. Sushi is a famous fish and rice food.\n",
      "['island', 'country', 'japan', 'great', 'economy', 'world_war_2', 'japan', 'sea', 'source', 'fish', 'sushi', 'famous', 'fish', 'rice', 'food']\n",
      "[['island', 'country', 'japan', 'great', 'economy', 'world_war_2'], ['japan', 'sea', 'source', 'fish'], ['sushi', 'famous', 'fish', 'rice', 'food']]\n",
      "{('country', 'island'): 1, ('island', 'japan'): 1, ('country', 'japan'): 2, ('country', 'great'): 1, ('great', 'japan'): 2, ('economy', 'japan'): 1, ('economy', 'great'): 2, ('great', 'world_war_2'): 1, ('economy', 'world_war_2'): 1, ('japan', 'sea'): 1, ('japan', 'source'): 1, ('sea', 'source'): 2, ('fish', 'sea'): 1, ('fish', 'source'): 1, ('famous', 'sushi'): 1, ('fish', 'sushi'): 1, ('famous', 'fish'): 2, ('famous', 'rice'): 1, ('fish', 'rice'): 2, ('fish', 'food'): 1, ('food', 'rice'): 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NodeView(('food', 'sushi', 'rice', 'country', 'world_war_2', 'famous', 'island', 'fish', 'sea', 'source', 'economy', 'japan', 'great'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# abstract = load_abstract('ex1')\n",
    "# joined, abstracts, titles, keywords = get_data('data/CS_papers.csv', version=\"CS\")\n",
    "# abstracts, keywords = get_data('data/inspec_papers.csv', version=\"inspec\")\n",
    "# abstract = joined[12]\n",
    "abstract = \"The island country of Japan has developed into a great economy after World War 2. \\\n",
    "The Japan sea is a source of fish. Sushi is a famous fish and rice food.\"\n",
    "ke = KeywordExtractor(abstract=abstract)\n",
    "print(abstract)\n",
    "print(ke.tokens)\n",
    "print(ke.sentences)\n",
    "print(ke.co)\n",
    "ke.graph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing third party keyword extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "# Extract keywords using TextRank\n",
    "extracted_keywords = keywords(abstract, lemmatize=True)\n",
    "\n",
    "# Print the extracted keywords\n",
    "print(extracted_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = abstract\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "#print(f\"Ground truth {keywords[0]}\")\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "{('food', 'fish'): 1, ('food', 'rice'): 1, ('sushi', 'famous'): 1, ('sushi', 'fish'): 1, ('rice', 'famous'): 1, ('rice', 'fish'): 2, ('country', 'island'): 1, ('country', 'japan'): 2, ('country', 'great'): 1, ('world_war_2', 'great'): 1, ('world_war_2', 'economy'): 1, ('famous', 'fish'): 2, ('island', 'japan'): 1, ('fish', 'sea'): 1, ('fish', 'source'): 1, ('sea', 'japan'): 1, ('sea', 'source'): 2, ('source', 'japan'): 1, ('economy', 'japan'): 1, ('economy', 'great'): 2, ('japan', 'great'): 2}\n",
      "Method selected: degree_centrality\n",
      "['fish', 'japan', 'great', 'rice', 'country']\n"
     ]
    }
   ],
   "source": [
    "print(len(ke.unique_tokens))\n",
    "labels = nx.get_edge_attributes(ke.graph,'weight')\n",
    "# ke.add_we_weights()\n",
    "#labels = nx.get_edge_attributes(ke.graph,'weight')\n",
    "\n",
    "# print(ke.graph.edges)\n",
    "print(labels)\n",
    "methods = {1: \"degree_centrality\", 2: \"closeness_centrality\", 3: \"betweenness_centrality\", 4:\"eigenvector_centrality\", 5:\"pagerank\", \\\n",
    "           6:\"katz_centrality\"} #, 7: \"hits\"}\n",
    "keyword_dict = ke.order_nodes(method=methods[1], to_print=False)\n",
    "predicted_keywords = list(keyword_dict.keys())[:5]\n",
    "print(predicted_keywords)\n",
    "# make_metrics_csv(ke, methods)\n",
    "# print(nx.shortest_path(ke.graph, source=\"rice\", target=\"japan\", weight=\"weight\"))\n",
    "# print(nx.shortest_path_length(ke.graph, source=\"world_war_2\", target=\"japan\", weight=\"weight\"))\n",
    "# ke.visualize_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centrality of node A: 0.33333\n",
      "Centrality of node B: 0.00000\n",
      "Centrality of node C: 0.00000\n",
      "Centrality of node D: 0.50000\n",
      "Centrality of node E: 0.00000\n",
      "[8.99852725 4.51399116 9.35935199 7.4005891  4.28571429]\n",
      "[0.55584268 0.27883107 0.57813097 0.45713739 0.26473031]\n"
     ]
    }
   ],
   "source": [
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "G.add_node('A')\n",
    "G.add_node('B')\n",
    "G.add_node('C')\n",
    "G.add_node('D')\n",
    "G.add_node('E')\n",
    "# Add edges\n",
    "G.add_edge('A', 'B', weight=1)\n",
    "G.add_edge('A', 'C', weight=3)\n",
    "G.add_edge('B', 'E', weight=2)\n",
    "G.add_edge('C', 'D', weight=2)\n",
    "G.add_edge('D', 'E', weight=1)\n",
    "G.add_edge('A', 'D', weight=1)\n",
    "\n",
    "# Calculate degree centrality\n",
    "d = 1\n",
    "\n",
    "degree_centrality = nx.katz_centrality(G, weight=\"weight\", alpha = 0.20, beta=0.1)\n",
    "degree_centrality = nx.betweenness_centrality(G, weight=\"weight\")\n",
    "\n",
    "# Print degree centrality for each node\n",
    "for node, centrality in degree_centrality.items():\n",
    "    print(f'Centrality of node {node}: {centrality:.5f}')\n",
    "\n",
    "A = nx.adjacency_matrix(G).toarray()\n",
    "I = np.eye(5)\n",
    "alpha = 0.2\n",
    "C = (np.linalg.inv(I - alpha * A)) @ np.ones(5)\n",
    "#C = (np.linalg.inv(I - alpha * A.T) - I) @ np.ones(5)\n",
    "print(C)\n",
    "print(C / np.sqrt(np.sum(C**2)))\n",
    "\n",
    "#print(A)\n",
    "# A = A / A.sum(axis=0, keepdims=True)\n",
    "# print(A)\n",
    "# eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "# print(eigenvalues)\n",
    "# dominant_index = np.argmax(eigenvalues)\n",
    "# print(dominant_index)\n",
    "# dominant_eigenvector = eigenvectors[:, dominant_index]\n",
    "# print(dominant_eigenvector)\n",
    "# print(dominant_eigenvector/np.sum(dominant_eigenvector))\n",
    "\n",
    "# I = np.eye(5)\n",
    "# R = (1-d)/5 * np.linalg.inv(I - d*np.transpose(A)) @ np.ones(5) \n",
    "# print(f\"final: {R}\")\n",
    "# Draw the graph\n",
    "# pos = nx.spring_layout(G)\n",
    "# nx.draw(G, pos, with_labels=True, font_weight='bold', node_color='skyblue', node_size=800)\n",
    "# edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = KeyedVectors.load('data/model.model')\n",
    "a = get_word_em(\"nlp\")\n",
    "b = get_word_em(\"apple\")\n",
    "print(model.similarity(\"nlp\", \"apple\"))\n",
    "print(cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0])\n",
    "similar_words = model.similar_by_vector(a, topn=5)\n",
    "print(\"\\nWords similar to:\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'apple':\n",
      "[ 0.52042  -0.8314    0.49961   1.2893    0.1151    0.057521 -1.3753\n",
      " -0.97313   0.18346   0.47672  -0.15112   0.35532   0.25912  -0.77857\n",
      "  0.52181   0.47695  -1.4251    0.858     0.59821  -1.0903    0.33574\n",
      " -0.60891   0.41742   0.21569  -0.07417  -0.5822   -0.4502    0.17253\n",
      "  0.16448  -0.38413   2.3283   -0.66682  -0.58181   0.74389   0.095015\n",
      " -0.47865  -0.84591   0.38704   0.23693  -1.5523    0.64802  -0.16521\n",
      " -1.4719   -0.16224   0.79857   0.97391   0.40027  -0.21912  -0.30938\n",
      "  0.26581 ]\n",
      "\n",
      "Words similar to 'apple':\n",
      "blackberry: 0.7543067932128906\n",
      "chips: 0.7438643574714661\n",
      "iphone: 0.7429664134979248\n",
      "microsoft: 0.7334205508232117\n",
      "ipad: 0.7331036925315857\n",
      "\n",
      "'king' - 'man' + 'woman' =\n",
      "queen: 0.8523604273796082\n",
      "Similarity between 'learning' and 'island': 0.25\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load('data/model.model')\n",
    "# Find the vector representation of a word\n",
    "word_vector = model[\"apple\"]\n",
    "print(\"Vector representation of 'apple':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.most_similar(\"apple\", topn=5)\n",
    "print(\"\\nWords similar to 'apple':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Perform vector arithmetic (e.g., king - man + woman = queen)\n",
    "result = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n",
    "print(\"\\n'king' - 'man' + 'woman' =\")\n",
    "for word, score in result:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Assuming you have already loaded a pretrained Word2Vec model\n",
    "word1 = \"learning\"\n",
    "word2 = \"island\"\n",
    "\n",
    "# Calculate the similarity between two words\n",
    "similarity_score = model.similarity(word1, word2)\n",
    "\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
