{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nlp_utils import *\n",
    "from data_utils import *\n",
    "from KeywordExtractor import *\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import json\n",
    "\n",
    "methods = {1: \"degree_centrality\", 2: \"closeness_centrality\", 3: \"betweenness_centrality\", 4:\"eigenvector_centrality\", 5:\"pagerank\"} #, \\\n",
    "           #6:\"katz_centrality\"} #, 7: \"hits\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD EMBEDDINGS\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "# Download a pre-trained Word2Vec model (you can choose other models as well)\n",
    "model_name = \"glove-wiki-gigaword-50\"\n",
    "\n",
    "# Download and load the model (this might take a while to download)\n",
    "# model = api.load(model_name)\n",
    "# model.save('data/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = 10\n",
    "json_file_path = f'data/mean_metrics_{ws}.json'\n",
    "\n",
    "# Open the file and load its contents into a dictionary\n",
    "with open(json_file_path, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "new_dict = {'Precision': [], 'Recall': [], \"F-score\": []}\n",
    "for key in json_data.keys():\n",
    "    if key[-1] == \"p\":\n",
    "        new_dict['Precision'].append(round(json_data[key]*100, 2))\n",
    "    elif key[-1] == \"r\":\n",
    "        new_dict['Recall'].append(round(json_data[key]*100, 2))\n",
    "    elif key[-1] == \"f\":\n",
    "        new_dict['F-score'].append(round(json_data[key]*100, 2))\n",
    "df = pd.DataFrame(new_dict)\n",
    "df.to_csv(f'data/m_{ws}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5000 \n",
      "Recall: 0.6000 \n",
      "F-score: 0.5455\n"
     ]
    }
   ],
   "source": [
    "true_labels = ['Worlds', 'b', 'c', 'd', 'e']\n",
    "predicted_labels = ['world_war_2', 'c', 'j', 'k', 'e', 'op']\n",
    "p, r, f = get_prf(true_labels, predicted_labels)\n",
    "print(f'Precision: {p:.4f} \\nRecall: {r:.4f} \\nF-score: {f:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/CS_unique.csv'\n",
    "\n",
    "#joined, abstracts, titles, keywords = get_data(path_to_file, version=\"CS\")\n",
    "metrics_dict = make_keyword_metrics(methods, path_to_file, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"The island country of Japan has developed into a great economy after World War 2. \\\n",
    "The Japan sea is a source of fish. Sushi is a famous fish and rice food.\"\n",
    "tokens, sentences = prune_text(abstract)\n",
    "co, index_dict = get_co(sentences, representation='matrix')\n",
    "# print(index_dict)\n",
    "# print(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the era of information overload it is essential to efficiently extract concise, precise and quality information from large texts. One aspect of information extraction is keyword extraction\n",
      "where large texts are represented as sets of keywords. This prospect of keyword extraction is\n",
      "paramount to researchers as they deal with huge numbers of scientific papers, and having a\n",
      "good and concise representation of those papers is essential for them. This thesis addresses that\n",
      "problem in the realm of Natural Language Processing (NLP).\n",
      "Using core NLP concepts and modeling texts as graphs, we are going to build a model for the\n",
      "automatic extraction of keywords. This is done in an unsupervised manner as the importance of\n",
      "a word is calculated through the position and weights associated with respective words in the\n",
      "graph. The first metric used to calculate the graph weights are co-occurrence matrices and the\n",
      "other metric are word embeddings. Word embeddings became a crucial way of representing the\n",
      "semantic information of words as dense vectors.\n",
      "The results of this paper were compared with keywords that were provided by authors of scientific papers in the area of computer science which act as the ground truth, but crucially are not\n",
      "a component in the model construction, but just serve as a verifier of the modelâ€™s accuracy.\n",
      "['era', 'information', 'overload', 'essential', 'concise', 'precise', 'quality', 'information', 'large', 'text', 'aspect', 'information', 'extraction', 'keyword', 'extraction', 'large', 'text', 'set', 'keywords', 'prospect', 'keyword', 'extraction', 'researcher', 'huge', 'number', 'scientific', 'paper', 'good', 'concise', 'representation', 'paper', 'essential', 'thesis', 'problem', 'realm', 'nlp', 'core', 'nlp', 'concept', 'text', 'graph', 'model', 'automatic', 'extraction', 'keywords', 'unsupervised', 'manner', 'importance', 'word', 'position', 'weight', 'respective', 'word', 'graph', 'first', 'metric', 'graph', 'weight', 'co-occurrence', 'matrix', 'metric', 'word_embeddings', 'word', 'crucial', 'way', 'semantic', 'information', 'word', 'dense', 'vector', 'result', 'paper', 'keywords', 'author', 'scientific', 'paper', 'area', 'computer', 'science', 'act', 'ground', 'truth', 'component', 'model', 'construction', 'verifier', 'accuracy']\n",
      "[['era', 'information', 'overload', 'essential', 'concise', 'precise', 'quality', 'information', 'large', 'text'], ['aspect', 'information', 'extraction', 'keyword', 'extraction', 'large', 'text', 'set', 'keywords'], ['prospect', 'keyword', 'extraction', 'researcher', 'huge', 'number', 'scientific', 'paper', 'good', 'concise', 'representation', 'paper', 'essential'], ['thesis', 'problem', 'realm', 'nlp'], ['core', 'nlp', 'concept', 'text', 'graph', 'model', 'automatic', 'extraction', 'keywords'], ['unsupervised', 'manner', 'importance', 'word', 'position', 'weight', 'respective', 'word', 'graph'], ['first', 'metric', 'graph', 'weight', 'co-occurrence', 'matrix', 'metric', 'word_embeddings'], ['word', 'crucial', 'way', 'semantic', 'information', 'word', 'dense', 'vector'], ['result', 'paper', 'keywords', 'author', 'scientific', 'paper', 'area', 'computer', 'science', 'act', 'ground', 'truth', 'component', 'model', 'construction', 'verifier', 'accuracy']]\n",
      "{('era', 'information'): 1, ('era', 'overload'): 1, ('information', 'overload'): 2, ('essential', 'information'): 1, ('essential', 'overload'): 2, ('concise', 'overload'): 1, ('concise', 'essential'): 2, ('essential', 'precise'): 1, ('concise', 'precise'): 2, ('concise', 'quality'): 1, ('precise', 'quality'): 2, ('information', 'precise'): 1, ('information', 'quality'): 2, ('large', 'quality'): 1, ('information', 'large'): 2, ('information', 'text'): 1, ('large', 'text'): 3, ('aspect', 'information'): 1, ('aspect', 'extraction'): 1, ('extraction', 'information'): 2, ('information', 'keyword'): 1, ('extraction', 'keyword'): 6, ('extraction', 'extraction'): 1, ('keyword', 'large'): 1, ('extraction', 'large'): 2, ('extraction', 'text'): 1, ('large', 'set'): 1, ('set', 'text'): 2, ('keywords', 'text'): 1, ('keywords', 'set'): 1, ('keyword', 'prospect'): 1, ('extraction', 'prospect'): 1, ('keyword', 'researcher'): 1, ('extraction', 'researcher'): 2, ('extraction', 'huge'): 1, ('huge', 'researcher'): 2, ('number', 'researcher'): 1, ('huge', 'number'): 2, ('huge', 'scientific'): 1, ('number', 'scientific'): 2, ('number', 'paper'): 1, ('paper', 'scientific'): 4, ('good', 'scientific'): 1, ('good', 'paper'): 2, ('concise', 'paper'): 2, ('concise', 'good'): 2, ('good', 'representation'): 1, ('concise', 'representation'): 2, ('paper', 'representation'): 2, ('essential', 'representation'): 1, ('essential', 'paper'): 1, ('problem', 'thesis'): 1, ('realm', 'thesis'): 1, ('problem', 'realm'): 2, ('nlp', 'problem'): 1, ('nlp', 'realm'): 1, ('core', 'nlp'): 1, ('concept', 'core'): 1, ('concept', 'nlp'): 2, ('nlp', 'text'): 1, ('concept', 'text'): 2, ('concept', 'graph'): 1, ('graph', 'text'): 2, ('model', 'text'): 1, ('graph', 'model'): 2, ('automatic', 'graph'): 1, ('automatic', 'model'): 2, ('extraction', 'model'): 1, ('automatic', 'extraction'): 2, ('automatic', 'keywords'): 1, ('extraction', 'keywords'): 1, ('manner', 'unsupervised'): 1, ('importance', 'unsupervised'): 1, ('importance', 'manner'): 2, ('manner', 'word'): 1, ('importance', 'word'): 2, ('importance', 'position'): 1, ('position', 'word'): 2, ('weight', 'word'): 2, ('position', 'weight'): 2, ('position', 'respective'): 1, ('respective', 'weight'): 2, ('respective', 'word'): 2, ('graph', 'respective'): 1, ('graph', 'word'): 1, ('first', 'metric'): 1, ('first', 'graph'): 1, ('graph', 'metric'): 2, ('metric', 'weight'): 1, ('graph', 'weight'): 2, ('co-occurrence', 'graph'): 1, ('co-occurrence', 'weight'): 2, ('matrix', 'weight'): 1, ('co-occurrence', 'matrix'): 2, ('co-occurrence', 'metric'): 1, ('matrix', 'metric'): 2, ('matrix', 'word_embeddings'): 1, ('metric', 'word_embeddings'): 1, ('crucial', 'word'): 1, ('way', 'word'): 1, ('crucial', 'way'): 2, ('crucial', 'semantic'): 1, ('semantic', 'way'): 2, ('information', 'way'): 1, ('information', 'semantic'): 2, ('semantic', 'word'): 1, ('information', 'word'): 2, ('dense', 'information'): 1, ('dense', 'word'): 2, ('vector', 'word'): 1, ('dense', 'vector'): 1, ('paper', 'result'): 1, ('keywords', 'result'): 1, ('keywords', 'paper'): 2, ('author', 'paper'): 2, ('author', 'keywords'): 2, ('keywords', 'scientific'): 1, ('author', 'scientific'): 2, ('area', 'scientific'): 1, ('area', 'paper'): 2, ('computer', 'paper'): 1, ('area', 'computer'): 2, ('area', 'science'): 1, ('computer', 'science'): 2, ('act', 'computer'): 1, ('act', 'science'): 2, ('ground', 'science'): 1, ('act', 'ground'): 2, ('act', 'truth'): 1, ('ground', 'truth'): 2, ('component', 'ground'): 1, ('component', 'truth'): 2, ('model', 'truth'): 1, ('component', 'model'): 2, ('component', 'construction'): 1, ('construction', 'model'): 2, ('model', 'verifier'): 1, ('construction', 'verifier'): 2, ('accuracy', 'construction'): 1, ('accuracy', 'verifier'): 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NodeView(('realm', 'science', 'unsupervised', 'thesis', 'aspect', 'manner', 'number', 'researcher', 'representation', 'ground', 'graph', 'concise', 'quality', 'keyword', 'prospect', 'position', 'huge', 'accuracy', 'co-occurrence', 'scientific', 'word_embeddings', 'semantic', 'essential', 'core', 'word', 'vector', 'truth', 'automatic', 'construction', 'component', 'good', 'importance', 'respective', 'precise', 'model', 'crucial', 'area', 'act', 'information', 'weight', 'overload', 'metric', 'author', 'problem', 'matrix', 'result', 'computer', 'way', 'era', 'large', 'first', 'verifier', 'keywords', 'concept', 'set', 'extraction', 'dense', 'nlp', 'text', 'paper'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# abstract = load_abstract('ex1')\n",
    "# joined, abstracts, titles, keywords = get_data('data/CS_papers.csv', version=\"CS\")\n",
    "# abstracts, keywords = get_data('data/inspec_papers.csv', version=\"inspec\")\n",
    "# abstract = joined[12]\n",
    "abstract = \"The island country of Japan has developed into a great economy after World War 2. \\\n",
    "The Japan sea is a source of fish. Sushi is a famous fish and rice food.\"\n",
    "abstract = load_abstract('ex1')\n",
    "ke = KeywordExtractor(abstract=abstract, window_size=3)\n",
    "print(abstract)\n",
    "print(ke.tokens)\n",
    "print(ke.sentences)\n",
    "print(ke.co)\n",
    "ke.graph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing third party keyword extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "# Extract keywords using TextRank\n",
    "extracted_keywords = keywords(abstract, lemmatize=True)\n",
    "\n",
    "# Print the extracted keywords\n",
    "print(extracted_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = abstract\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "#print(f\"Ground truth {keywords[0]}\")\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "{('realm', 'thesis'): 1, ('realm', 'problem'): 2, ('realm', 'nlp'): 1, ('science', 'area'): 1, ('science', 'computer'): 2, ('science', 'act'): 2, ('science', 'ground'): 1, ('unsupervised', 'manner'): 1, ('unsupervised', 'importance'): 1, ('thesis', 'problem'): 1, ('aspect', 'information'): 1, ('aspect', 'extraction'): 1, ('manner', 'importance'): 2, ('manner', 'word'): 1, ('number', 'researcher'): 1, ('number', 'huge'): 2, ('number', 'scientific'): 2, ('number', 'paper'): 1, ('researcher', 'keyword'): 1, ('researcher', 'extraction'): 2, ('researcher', 'huge'): 2, ('representation', 'good'): 1, ('representation', 'concise'): 2, ('representation', 'paper'): 2, ('representation', 'essential'): 1, ('ground', 'act'): 2, ('ground', 'truth'): 2, ('ground', 'component'): 1, ('graph', 'concept'): 1, ('graph', 'text'): 2, ('graph', 'model'): 2, ('graph', 'automatic'): 1, ('graph', 'respective'): 1, ('graph', 'word'): 1, ('graph', 'first'): 1, ('graph', 'metric'): 2, ('graph', 'weight'): 2, ('graph', 'co-occurrence'): 1, ('concise', 'overload'): 1, ('concise', 'essential'): 2, ('concise', 'precise'): 2, ('concise', 'quality'): 1, ('concise', 'paper'): 2, ('concise', 'good'): 2, ('quality', 'precise'): 2, ('quality', 'information'): 2, ('quality', 'large'): 1, ('keyword', 'information'): 1, ('keyword', 'extraction'): 6, ('keyword', 'large'): 1, ('keyword', 'prospect'): 1, ('prospect', 'extraction'): 1, ('position', 'importance'): 1, ('position', 'word'): 2, ('position', 'weight'): 2, ('position', 'respective'): 1, ('huge', 'extraction'): 1, ('huge', 'scientific'): 1, ('accuracy', 'construction'): 1, ('accuracy', 'verifier'): 1, ('co-occurrence', 'weight'): 2, ('co-occurrence', 'matrix'): 2, ('co-occurrence', 'metric'): 1, ('scientific', 'paper'): 4, ('scientific', 'good'): 1, ('scientific', 'keywords'): 1, ('scientific', 'author'): 2, ('scientific', 'area'): 1, ('word_embeddings', 'matrix'): 1, ('word_embeddings', 'metric'): 1, ('semantic', 'crucial'): 1, ('semantic', 'way'): 2, ('semantic', 'information'): 2, ('semantic', 'word'): 1, ('essential', 'information'): 1, ('essential', 'overload'): 2, ('essential', 'precise'): 1, ('essential', 'paper'): 1, ('core', 'nlp'): 1, ('core', 'concept'): 1, ('word', 'importance'): 2, ('word', 'weight'): 2, ('word', 'respective'): 2, ('word', 'crucial'): 1, ('word', 'way'): 1, ('word', 'information'): 2, ('word', 'dense'): 2, ('word', 'vector'): 1, ('vector', 'dense'): 1, ('truth', 'act'): 1, ('truth', 'component'): 2, ('truth', 'model'): 1, ('automatic', 'model'): 2, ('automatic', 'extraction'): 2, ('automatic', 'keywords'): 1, ('construction', 'component'): 1, ('construction', 'model'): 2, ('construction', 'verifier'): 2, ('component', 'model'): 2, ('good', 'paper'): 2, ('respective', 'weight'): 2, ('precise', 'information'): 1, ('model', 'text'): 1, ('model', 'extraction'): 1, ('model', 'verifier'): 1, ('crucial', 'way'): 2, ('area', 'paper'): 2, ('area', 'computer'): 2, ('act', 'computer'): 1, ('information', 'era'): 1, ('information', 'overload'): 2, ('information', 'large'): 2, ('information', 'text'): 1, ('information', 'extraction'): 2, ('information', 'way'): 1, ('information', 'dense'): 1, ('weight', 'metric'): 1, ('weight', 'matrix'): 1, ('overload', 'era'): 1, ('metric', 'first'): 1, ('metric', 'matrix'): 2, ('author', 'paper'): 2, ('author', 'keywords'): 2, ('problem', 'nlp'): 1, ('result', 'paper'): 1, ('result', 'keywords'): 1, ('computer', 'paper'): 1, ('large', 'text'): 3, ('large', 'extraction'): 2, ('large', 'set'): 1, ('keywords', 'text'): 1, ('keywords', 'set'): 1, ('keywords', 'extraction'): 1, ('keywords', 'paper'): 2, ('concept', 'nlp'): 2, ('concept', 'text'): 2, ('set', 'text'): 2, ('extraction', 'extraction'): 1, ('extraction', 'text'): 1, ('nlp', 'text'): 1}\n",
      "['information', 'extraction', 'word', 'paper', 'graph', 'text', 'model', 'weight']\n"
     ]
    }
   ],
   "source": [
    "print(len(ke.unique_tokens))\n",
    "labels = nx.get_edge_attributes(ke.graph,'weight')\n",
    "# ke.add_we_weights()\n",
    "#labels = nx.get_edge_attributes(ke.graph,'weight')\n",
    "\n",
    "# print(ke.graph.edges)\n",
    "print(labels)\n",
    "methods = {1: \"degree_centrality\", 2: \"closeness_centrality\", 3: \"betweenness_centrality\", 4:\"eigenvector_centrality\", 5:\"pagerank\", \\\n",
    "           6:\"katz_centrality\"} #, 7: \"hits\"}\n",
    "keyword_dict = ke.order_nodes(method=methods[5], to_print=False)\n",
    "predicted_keywords = list(keyword_dict.keys())[:8]\n",
    "print(predicted_keywords)\n",
    "# make_metrics_csv(ke, methods)\n",
    "# print(nx.shortest_path(ke.graph, source=\"rice\", target=\"japan\", weight=\"weight\"))\n",
    "# print(nx.shortest_path_length(ke.graph, source=\"world_war_2\", target=\"japan\", weight=\"weight\"))\n",
    "# ke.visualize_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centrality of node A: 0.33333\n",
      "Centrality of node B: 0.00000\n",
      "Centrality of node C: 0.00000\n",
      "Centrality of node D: 0.50000\n",
      "Centrality of node E: 0.00000\n",
      "[8.99852725 4.51399116 9.35935199 7.4005891  4.28571429]\n",
      "[0.55584268 0.27883107 0.57813097 0.45713739 0.26473031]\n"
     ]
    }
   ],
   "source": [
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "G.add_node('A')\n",
    "G.add_node('B')\n",
    "G.add_node('C')\n",
    "G.add_node('D')\n",
    "G.add_node('E')\n",
    "# Add edges\n",
    "G.add_edge('A', 'B', weight=1)\n",
    "G.add_edge('A', 'C', weight=3)\n",
    "G.add_edge('B', 'E', weight=2)\n",
    "G.add_edge('C', 'D', weight=2)\n",
    "G.add_edge('D', 'E', weight=1)\n",
    "G.add_edge('A', 'D', weight=1)\n",
    "\n",
    "# Calculate degree centrality\n",
    "d = 1\n",
    "\n",
    "degree_centrality = nx.katz_centrality(G, weight=\"weight\", alpha = 0.20, beta=0.1)\n",
    "degree_centrality = nx.betweenness_centrality(G, weight=\"weight\")\n",
    "\n",
    "# Print degree centrality for each node\n",
    "for node, centrality in degree_centrality.items():\n",
    "    print(f'Centrality of node {node}: {centrality:.5f}')\n",
    "\n",
    "A = nx.adjacency_matrix(G).toarray()\n",
    "I = np.eye(5)\n",
    "alpha = 0.2\n",
    "C = (np.linalg.inv(I - alpha * A)) @ np.ones(5)\n",
    "#C = (np.linalg.inv(I - alpha * A.T) - I) @ np.ones(5)\n",
    "print(C)\n",
    "print(C / np.sqrt(np.sum(C**2)))\n",
    "\n",
    "#print(A)\n",
    "# A = A / A.sum(axis=0, keepdims=True)\n",
    "# print(A)\n",
    "# eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "# print(eigenvalues)\n",
    "# dominant_index = np.argmax(eigenvalues)\n",
    "# print(dominant_index)\n",
    "# dominant_eigenvector = eigenvectors[:, dominant_index]\n",
    "# print(dominant_eigenvector)\n",
    "# print(dominant_eigenvector/np.sum(dominant_eigenvector))\n",
    "\n",
    "# I = np.eye(5)\n",
    "# R = (1-d)/5 * np.linalg.inv(I - d*np.transpose(A)) @ np.ones(5) \n",
    "# print(f\"final: {R}\")\n",
    "# Draw the graph\n",
    "# pos = nx.spring_layout(G)\n",
    "# nx.draw(G, pos, with_labels=True, font_weight='bold', node_color='skyblue', node_size=800)\n",
    "# edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = KeyedVectors.load('data/model.model')\n",
    "a = get_word_em(\"nlp\")\n",
    "b = get_word_em(\"apple\")\n",
    "print(model.similarity(\"nlp\", \"apple\"))\n",
    "print(cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0])\n",
    "similar_words = model.similar_by_vector(a, topn=5)\n",
    "print(\"\\nWords similar to:\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'apple':\n",
      "[ 0.52042  -0.8314    0.49961   1.2893    0.1151    0.057521 -1.3753\n",
      " -0.97313   0.18346   0.47672  -0.15112   0.35532   0.25912  -0.77857\n",
      "  0.52181   0.47695  -1.4251    0.858     0.59821  -1.0903    0.33574\n",
      " -0.60891   0.41742   0.21569  -0.07417  -0.5822   -0.4502    0.17253\n",
      "  0.16448  -0.38413   2.3283   -0.66682  -0.58181   0.74389   0.095015\n",
      " -0.47865  -0.84591   0.38704   0.23693  -1.5523    0.64802  -0.16521\n",
      " -1.4719   -0.16224   0.79857   0.97391   0.40027  -0.21912  -0.30938\n",
      "  0.26581 ]\n",
      "\n",
      "Words similar to 'apple':\n",
      "blackberry: 0.7543067932128906\n",
      "chips: 0.7438643574714661\n",
      "iphone: 0.7429664134979248\n",
      "microsoft: 0.7334205508232117\n",
      "ipad: 0.7331036925315857\n",
      "\n",
      "'king' - 'man' + 'woman' =\n",
      "queen: 0.8523604273796082\n",
      "Similarity between 'learning' and 'island': 0.25\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load('data/model.model')\n",
    "# Find the vector representation of a word\n",
    "word_vector = model[\"apple\"]\n",
    "print(\"Vector representation of 'apple':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.most_similar(\"apple\", topn=5)\n",
    "print(\"\\nWords similar to 'apple':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Perform vector arithmetic (e.g., king - man + woman = queen)\n",
    "result = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n",
    "print(\"\\n'king' - 'man' + 'woman' =\")\n",
    "for word, score in result:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Assuming you have already loaded a pretrained Word2Vec model\n",
    "word1 = \"learning\"\n",
    "word2 = \"island\"\n",
    "\n",
    "# Calculate the similarity between two words\n",
    "similarity_score = model.similarity(word1, word2)\n",
    "\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
